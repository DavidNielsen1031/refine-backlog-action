name: "Refine Backlog"
description: "Transform messy backlog items into structured, actionable work items using AI"
branding:
  icon: "check-square"
  color: "blue"

inputs:
  items:
    description: "Backlog items to refine (newline-separated). Use either this OR file."
    required: false
  file:
    description: "Path to a file containing backlog items (one per line)"
    required: false
  key:
    description: "Refine Backlog license key. Can also be set via REFINE_BACKLOG_KEY env var."
    required: false
  user-stories:
    description: "Add a user story title to each refined item"
    required: false
    default: "false"
  gherkin:
    description: "Write acceptance criteria in Given/When/Then format"
    required: false
    default: "false"
  context:
    description: "Project context to guide the AI (e.g. 'B2B SaaS, TypeScript, Agile team of 5')"
    required: false
  output-file:
    description: "Write refined JSON output to this file path (e.g. refined.json)"
    required: false
  write-back:
    description: "Write refined output back to the GitHub issue as a comment (requires issues: write permission)"
    required: false
    default: "false"

outputs:
  refined:
    description: "JSON string of all refined backlog items"
    value: ${{ steps.refine.outputs.refined }}
  count:
    description: "Number of items successfully refined"
    value: ${{ steps.refine.outputs.count }}
  refined_file:
    description: "Path to a temp file containing the refined JSON (safe for downstream shell use)"
    value: ${{ steps.refine.outputs.refined_file }}

runs:
  using: "composite"
  steps:
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"

    - name: Checkout (only when reading from a file)
      if: ${{ inputs.file != '' }}
      uses: actions/checkout@v4

    - name: Refine backlog items
      id: refine
      shell: bash
      env:
        REFINE_BACKLOG_KEY: ${{ inputs.key || env.REFINE_BACKLOG_KEY }}
        RB_CONTEXT: ${{ inputs.context }}
      run: |
        set -e

        # Auto-detect context if not provided explicitly
        # Explicit `context:` input always wins â€” skip detection entirely if set
        if [ -z "$RB_CONTEXT" ]; then
          _RB_SOURCES_FILE=$(mktemp)

          # Write the detection script to a temp file (avoids heredoc quoting issues in YAML)
          cat << 'DETECT_JSEOF' > /tmp/rb-detect.js
        const fs = require('fs');
        const sourcesFile = process.argv[2];
        const parts = [];
        const sources = [];
        
        // Read a file safely: truncate, strip chars that break shell eval (backticks, backslashes, newlines)
        function readSafe(path, maxChars) {
          try {
            return fs.readFileSync(path, 'utf8')
              .slice(0, maxChars)
              .replace(/[`\\\n\r]/g, ' ')
              .replace(/\s+/g, ' ')
              .trim();
          } catch(e) { return ''; }
        }
        
        // 1. AI agent files â€” take the FIRST found (highest signal), 400 chars
        const agentFiles = [
          'AGENTS.md', 'CLAUDE.md', 'CODEX.md', 'GEMINI.md',
          '.github/copilot-instructions.md', '.windsurfrules',
          '.aider.conf.yml', 'llms.txt'
        ];
        for (const f of agentFiles) {
          if (fs.existsSync(f)) {
            const chunk = readSafe(f, 400);
            if (chunk) { parts.push(chunk); sources.push(f); break; }
          }
        }
        
        // 2. Stack detection â€” smart extraction per file type, combine all hits
        const stackParts = [];
        
        // package.json: name + description + top 8 dep names
        if (fs.existsSync('package.json')) {
          try {
            const p = JSON.parse(fs.readFileSync('package.json', 'utf8'));
            const deps = Object.keys({...(p.dependencies||{}), ...(p.devDependencies||{})}).slice(0, 8).join(', ');
            const info = [p.name, p.description, deps ? 'deps: '+deps : ''].filter(Boolean).join('. ');
            if (info) { stackParts.push(info); sources.push('package.json'); }
          } catch(e) {}
        }
        
        // Framework/deploy presence signals (file existence = signal)
        if (['next.config.js','next.config.ts','next.config.mjs'].some(f => fs.existsSync(f))) {
          stackParts.push('Next.js');
          sources.push('next.config');
        }
        if (fs.existsSync('vercel.json')) { stackParts.push('Vercel'); sources.push('vercel.json'); }
        if (fs.existsSync('netlify.toml')) { stackParts.push('Netlify'); sources.push('netlify.toml'); }
        
        // Prisma schema â€” model names = product domain, very high signal
        if (fs.existsSync('prisma/schema.prisma')) {
          try {
            const schema = fs.readFileSync('prisma/schema.prisma', 'utf8').slice(0, 300);
            const models = [...schema.matchAll(/model\s+(\w+)\s*\{/g)].map(m => m[1]);
            if (models.length) { stackParts.push('Prisma models: '+models.join(', ')); sources.push('prisma/schema.prisma'); }
          } catch(e) {}
        }
        
        // app.json / eas.json â€” Expo detection
        if (fs.existsSync('app.json')) {
          try {
            const a = JSON.parse(fs.readFileSync('app.json', 'utf8'));
            const expo = a.expo || a;
            const plat = Array.isArray(expo.platforms) ? 'platforms:'+expo.platforms.join('/') : '';
            const info = ['Expo', expo.name, expo.slug, plat].filter(Boolean).join(', ');
            stackParts.push(info); sources.push('app.json');
          } catch(e) {}
        }
        
        // pyproject.toml â€” name + description only
        if (fs.existsSync('pyproject.toml')) {
          try {
            const c = fs.readFileSync('pyproject.toml', 'utf8');
            const name = (c.match(/name\s*=\s*["']([^"']+)["']/) || [])[1];
            const desc = (c.match(/description\s*=\s*["']([^"']+)["']/) || [])[1];
            if (name||desc) { stackParts.push('Python: '+[name,desc].filter(Boolean).join(', ')); sources.push('pyproject.toml'); }
          } catch(e) {}
        }
        
        // Cargo.toml â€” name + description
        if (fs.existsSync('Cargo.toml')) {
          try {
            const c = fs.readFileSync('Cargo.toml', 'utf8');
            const name = (c.match(/^name\s*=\s*["']([^"']+)["']/m) || [])[1];
            const desc = (c.match(/^description\s*=\s*["']([^"']+)["']/m) || [])[1];
            if (name||desc) { stackParts.push('Rust: '+[name,desc].filter(Boolean).join(', ')); sources.push('Cargo.toml'); }
          } catch(e) {}
        }
        
        // go.mod â€” module name only
        if (fs.existsSync('go.mod')) {
          try {
            const c = fs.readFileSync('go.mod', 'utf8');
            const mod = (c.match(/^module\s+(\S+)/m) || [])[1];
            if (mod) { stackParts.push('Go: '+mod); sources.push('go.mod'); }
          } catch(e) {}
        }
        
        // pubspec.yaml â€” name + description (Flutter/Dart)
        if (fs.existsSync('pubspec.yaml')) {
          try {
            const c = fs.readFileSync('pubspec.yaml', 'utf8');
            const name = (c.match(/^name:\s*(.+)/m) || [])[1];
            const desc = (c.match(/^description:\s*(.+)/m) || [])[1];
            if (name||desc) { stackParts.push('Flutter: '+[name,desc].filter(Boolean).join(', ').trim()); sources.push('pubspec.yaml'); }
          } catch(e) {}
        }
        
        if (stackParts.length) parts.push(stackParts.join(' | '));
        
        // 3. Documentation â€” first 300 chars of README, 200 of CONTRIBUTING
        if (fs.existsSync('README.md')) {
          const chunk = readSafe('README.md', 300);
          if (chunk) { parts.push(chunk); sources.push('README.md'); }
        }
        if (fs.existsSync('CONTRIBUTING.md')) {
          const chunk = readSafe('CONTRIBUTING.md', 200);
          if (chunk) { parts.push(chunk); sources.push('CONTRIBUTING.md'); }
        }
        
        // 4. Team size from CODEOWNERS
        if (fs.existsSync('.github/CODEOWNERS')) {
          try {
            const c = fs.readFileSync('.github/CODEOWNERS', 'utf8');
            const owners = new Set([...c.matchAll(/@([A-Za-z0-9_-]+)/g)].map(m => m[1]));
            parts.push(owners.size <= 1 ? 'solo developer' : 'team of '+owners.size);
            sources.push('CODEOWNERS');
          } catch(e) {}
        }
        
        // Combine all hits, cap at 700 chars, write sources to file
        const combined = parts.join(' ').slice(0, 700);
        const sourceList = [...new Set(sources)].join(', ');
        if (sourcesFile) {
          try { fs.writeFileSync(sourcesFile, sourceList); } catch(e) {}
        }
        process.stdout.write(combined);
        DETECT_JSEOF

          RB_CONTEXT=$(node /tmp/rb-detect.js "$_RB_SOURCES_FILE" 2>/dev/null || echo "")
          if [ -n "$RB_CONTEXT" ]; then
            echo "Auto-detected context from: $(cat $_RB_SOURCES_FILE) (${#RB_CONTEXT} chars)"
          fi
          rm -f /tmp/rb-detect.js "$_RB_SOURCES_FILE"
        fi

        # Build CLI argument list
        ARGS="--format json"

        if [ "${{ inputs.user-stories }}" = "true" ]; then
          ARGS="$ARGS --user-stories"
        fi
        if [ "${{ inputs.gherkin }}" = "true" ]; then
          ARGS="$ARGS --gherkin"
        fi
        # Pass context via env var to avoid shell word-splitting on spaces
        if [ -n "$RB_CONTEXT" ]; then
          ARGS="$ARGS --context \"$RB_CONTEXT\""
        fi

        # Handle inline items â€” write to temp file to avoid shell quoting issues
        if [ -n "${{ inputs.items }}" ]; then
          ITEMS_FILE=$(mktemp)
          printf '%s' "${{ inputs.items }}" > "$ITEMS_FILE"
          ARGS="$ARGS --file $ITEMS_FILE"
        elif [ -n "${{ inputs.file }}" ]; then
          ARGS="$ARGS --file ${{ inputs.file }}"
        else
          echo "::error::Either 'items' or 'file' input is required."
          exit 1
        fi

        # Run the CLI
        echo "::group::Refine Backlog output"
        REFINED=$(eval npx --yes refine-backlog-cli $ARGS)
        echo "::endgroup::"

        # Parse item count
        COUNT=$(echo "$REFINED" | node -e "
          let data = '';
          process.stdin.on('data', c => data += c);
          process.stdin.on('end', () => {
            try { const p = JSON.parse(data); console.log(Array.isArray(p) ? p.length : (p.items || []).length); }
            catch(e) { console.log(0); }
          });
        " 2>/dev/null || echo "0")

        echo "âœ… Refined $COUNT item(s)"

        # Write JSON to a stable temp file (safe for downstream steps â€” no quoting issues)
        REFINED_FILE=$(mktemp /tmp/refine-backlog-XXXXXX.json)
        echo "$REFINED" > "$REFINED_FILE"

        # Set outputs
        {
          echo "refined<<EOF"
          echo "$REFINED"
          echo "EOF"
          echo "count=$COUNT"
          echo "refined_file=$REFINED_FILE"
        } >> "$GITHUB_OUTPUT"

        # Write to user-specified output file if requested
        if [ -n "${{ inputs.output-file }}" ]; then
          cp "$REFINED_FILE" "${{ inputs.output-file }}"
          echo "ðŸ“„ Written to ${{ inputs.output-file }}"
        fi

    - name: Write back to issue
      if: ${{ inputs.write-back == 'true' && github.event_name == 'issues' }}
      shell: bash
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        ISSUE_NUMBER="${{ github.event.issue.number }}"
        REPO="${{ github.repository }}"

        # Read from the temp file written by the refine step â€” safe, no quoting issues
        REFINED_FILE="${{ steps.refine.outputs.refined_file }}"

        # Format the comment with Node reading from the temp file
        COMMENT=$(node -e "
          const fs = require('fs');
          const raw = JSON.parse(fs.readFileSync('$REFINED_FILE', 'utf8')); const items = Array.isArray(raw) ? raw : (raw.items || []);
          const lines = ['## ðŸ¤– Refine Backlog Results', ''];
          items.forEach((item, i) => {
            lines.push(\`### \${i + 1}. \${item.title}\`);
            lines.push('');
            lines.push(\`**Priority:** \${item.priority}\`);
            lines.push(\`**Estimate:** \${item.estimate}\`);
            lines.push('');
            lines.push(\`**Problem:** \${item.problem}\`);
            lines.push('');
            lines.push('**Acceptance Criteria:**');
            item.acceptanceCriteria.forEach(ac => lines.push(\`- \${ac}\`));
            if (item.userStory) {
              lines.push('');
              lines.push(\`**User Story:** \${item.userStory}\`);
            }
            if (item.tags?.length) {
              lines.push('');
              lines.push(\`**Tags:** \${item.tags.join(', ')}\`);
            }
            if (item.assumptions?.length) {
              lines.push('');
              lines.push('**Open Questions:**');
              item.assumptions.forEach(a => lines.push(\`- \${a}\`));
            }
            lines.push('');
            lines.push('---');
            lines.push('');
          });
          lines.push('_Powered by [Refine Backlog](https://refinebacklog.com)_');
          process.stdout.write(lines.join('\n'));
        ")

        gh issue comment "$ISSUE_NUMBER" --repo "$REPO" --body "$COMMENT"
        echo "âœ… Comment posted to issue #$ISSUE_NUMBER"
